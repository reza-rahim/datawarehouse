{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b2b05e4-1ff7-41b5-94d8-6ba8cc3f96df",
   "metadata": {},
   "source": [
    "### Create the spark context "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "87fbee65-e367-48fc-a537-3456a1216e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "import os\n",
    "\n",
    "secret = os.environ.get(\"SPARK_AUTH_SECRET\")\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.authenticate\", \"true\")\n",
    "conf.set(\"spark.driver.extraJavaOptions\", f\"-Dspark.authenticate.secret={secret}\")\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf)\\\n",
    "    .appName(\"merge_into_bronze\") \\\n",
    "    .master(\"spark://node1.dw.felicity.net.bd:7077,node2.dw.felicity.net.bd:7077\")\\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ef688-8419-487b-b486-9d50380afe1f",
   "metadata": {},
   "source": [
    "### Creating tmp view for landing file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bf3428ce-0b2b-4a69-968b-8bac8bcce17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### geo_location_csv\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TEMPORARY VIEW geo_location_csv(\n",
    "    location_id BIGINT, \n",
    "    country STRING, \n",
    "    state STRING, \n",
    "    city STRING, \n",
    "    postal_code STRING\n",
    ")\n",
    "USING csv\n",
    "OPTIONS (\n",
    "  path 's3a://spark-data/landing/geo_location.csv',\n",
    "  header 'true'\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "### customer_csv\n",
    "spark.sql(f\"\"\" \n",
    "CREATE OR REPLACE TEMPORARY VIEW customer_csv (\n",
    "    customer_id INT,\n",
    "    full_name STRING,\n",
    "    email STRING,\n",
    "    phone_number STRING,\n",
    "    location_id INT,\n",
    "    created_at TIMESTAMP\n",
    ")\n",
    "USING csv\n",
    "OPTIONS (\n",
    "  path 's3a://spark-data/landing/customer.csv',\n",
    "  header 'true'\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "### product_csv\n",
    "spark.sql(f\"\"\" \n",
    "CREATE OR REPLACE TEMPORARY VIEW product_csv (\n",
    "    product_id INT,\n",
    "    product_name STRING,\n",
    "    description STRING,\n",
    "    category STRING,\n",
    "    price DECIMAL(10,2),\n",
    "    in_stock BOOLEAN,\n",
    "    created_at TIMESTAMP\n",
    ")\n",
    "USING csv\n",
    "OPTIONS (\n",
    "  path 's3a://spark-data/landing/product.csv',\n",
    "  header 'true'\n",
    ");\n",
    "\"\"\");\n",
    "\n",
    "### sales_order_csv\n",
    "spark.sql(f\"\"\" \n",
    "CREATE OR REPLACE TEMPORARY VIEW sales_order_csv (\n",
    "    order_id INT,\n",
    "    customer_id INT,\n",
    "    order_date TIMESTAMP,\n",
    "    total_amount DECIMAL(12,2),\n",
    "    status STRING,\n",
    "    updated_at TIMESTAMP\n",
    ")\n",
    "USING csv\n",
    "OPTIONS (\n",
    "  path 's3a://spark-data/landing/sales_order.csv',\n",
    "  header 'true'\n",
    ");\n",
    "\"\"\");\n",
    "\n",
    "### order_item_csv\n",
    "spark.sql(f\"\"\" \n",
    "CREATE OR REPLACE TEMPORARY VIEW order_item_csv (\n",
    "    order_item_id INT,\n",
    "    order_id INT,\n",
    "    product_id INT,\n",
    "    quantity INT,\n",
    "    unit_price DECIMAL(10,2),\n",
    "    total_price DECIMAL(12,2)\n",
    ")\n",
    "USING csv\n",
    "OPTIONS (\n",
    "  path 's3a://spark-data/landing/order_item.csv',\n",
    "  header 'true'\n",
    ");\n",
    "\"\"\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4cc6c0-681f-4ddc-bd18-484d05a22782",
   "metadata": {},
   "source": [
    "### Merging landing data into bronze layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a9b713f5-8956-4cf0-9698-698d68706f98",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[TABLE_OR_VIEW_NOT_FOUND] The table or view `bronze`.`geo_location` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 2 pos 11;\n'MergeIntoTable ('target.location_id = 'source.location_id), [updateaction(None, assignment('target.country, 'source.country), assignment('target.state, 'source.state), assignment('target.city, 'source.city), assignment('target.postal_code, 'source.postal_code), assignment('target.load_timestamp, 'source.load_timestamp))], [insertaction(None, assignment('location_id, 'source.location_id), assignment('country, 'source.country), assignment('state, 'source.state), assignment('city, 'source.city), assignment('postal_code, 'source.postal_code), assignment('load_timestamp, 'source.load_timestamp))]\n:- 'SubqueryAlias target\n:  +- 'UnresolvedRelation [bronze, geo_location], [__required_write_privileges__=INSERT,UPDATE], false\n+- SubqueryAlias source\n   +- Project [location_id#1010L, country#1011, state#1012, city#1013, postal_code#1014, current_timestamp() AS load_timestamp#1070]\n      +- SubqueryAlias geo_location_csv\n         +- View (`geo_location_csv`, [location_id#1010L,country#1011,state#1012,city#1013,postal_code#1014])\n            +- Relation [location_id#1010L,country#1011,state#1012,city#1013,postal_code#1014] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;43mMERGE INTO bronze.geo_location AS target\u001b[39;49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;43mUSING (\u001b[39;49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;43m  SELECT *, current_timestamp() AS load_timestamp\u001b[39;49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;43m  FROM geo_location_csv\u001b[39;49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;43m) AS source\u001b[39;49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;43mON target.location_id = source.location_id\u001b[39;49m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;43mWHEN MATCHED THEN\u001b[39;49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;43m  UPDATE SET\u001b[39;49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;43m    target.country = source.country,\u001b[39;49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;43m    target.state = source.state,\u001b[39;49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;43m    target.city = source.city,\u001b[39;49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;43m    target.postal_code = source.postal_code,\u001b[39;49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;43m    target.load_timestamp = source.load_timestamp\u001b[39;49m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;43mWHEN NOT MATCHED THEN\u001b[39;49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;43m  INSERT (location_id, country, state, city, postal_code, load_timestamp)\u001b[39;49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;43m  VALUES (source.location_id, source.country, source.state, source.city, source.postal_code, source.load_timestamp);\u001b[39;49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m;\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `bronze`.`geo_location` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 2 pos 11;\n'MergeIntoTable ('target.location_id = 'source.location_id), [updateaction(None, assignment('target.country, 'source.country), assignment('target.state, 'source.state), assignment('target.city, 'source.city), assignment('target.postal_code, 'source.postal_code), assignment('target.load_timestamp, 'source.load_timestamp))], [insertaction(None, assignment('location_id, 'source.location_id), assignment('country, 'source.country), assignment('state, 'source.state), assignment('city, 'source.city), assignment('postal_code, 'source.postal_code), assignment('load_timestamp, 'source.load_timestamp))]\n:- 'SubqueryAlias target\n:  +- 'UnresolvedRelation [bronze, geo_location], [__required_write_privileges__=INSERT,UPDATE], false\n+- SubqueryAlias source\n   +- Project [location_id#1010L, country#1011, state#1012, city#1013, postal_code#1014, current_timestamp() AS load_timestamp#1070]\n      +- SubqueryAlias geo_location_csv\n         +- View (`geo_location_csv`, [location_id#1010L,country#1011,state#1012,city#1013,postal_code#1014])\n            +- Relation [location_id#1010L,country#1011,state#1012,city#1013,postal_code#1014] csv\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "MERGE INTO bronze.geo_location AS target\n",
    "USING (\n",
    "  SELECT *, current_timestamp() AS load_timestamp\n",
    "  FROM geo_location_csv\n",
    ") AS source\n",
    "ON target.location_id = source.location_id\n",
    "\n",
    "WHEN MATCHED THEN\n",
    "  UPDATE SET\n",
    "    target.country = source.country,\n",
    "    target.state = source.state,\n",
    "    target.city = source.city,\n",
    "    target.postal_code = source.postal_code,\n",
    "    target.load_timestamp = source.load_timestamp\n",
    "\n",
    "WHEN NOT MATCHED THEN\n",
    "  INSERT (location_id, country, state, city, postal_code, load_timestamp)\n",
    "  VALUES (source.location_id, source.country, source.state, source.city, source.postal_code, source.load_timestamp);\n",
    "\"\"\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7bee0fd3-84b4-4336-8082-a1ab99bb50ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|      15|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" select count(*) from  bronze.geo_location; \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5b2f6be1-5589-4951-bcb1-2618819212c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "MERGE INTO bronze.customer AS target\n",
    "USING (\n",
    "  SELECT *, current_timestamp() AS load_timestamp\n",
    "  FROM customer_csv\n",
    ") AS source\n",
    "ON target.customer_id = source.customer_id\n",
    "\n",
    "WHEN MATCHED THEN\n",
    "  UPDATE SET\n",
    "    full_name     = source.full_name,\n",
    "    email         = source.email,\n",
    "    phone_number  = source.phone_number,\n",
    "    location_id   = source.location_id,\n",
    "    created_at    = source.created_at,\n",
    "    load_timestamp = source.load_timestamp\n",
    "\n",
    "WHEN NOT MATCHED THEN\n",
    "  INSERT (\n",
    "    customer_id, full_name, email, phone_number, location_id, created_at, load_timestamp\n",
    "  )\n",
    "  VALUES (\n",
    "    source.customer_id, source.full_name, source.email, source.phone_number,\n",
    "    source.location_id, source.created_at, source.load_timestamp\n",
    "  );\n",
    "\n",
    "\"\"\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "576d4d26-5829-4ce3-8cbb-9edcb0b0d430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|     250|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" select count(*) from  bronze.customer; \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f1e699be-0be1-47d3-8cf0-41826f140fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "MERGE INTO bronze.product AS target\n",
    "USING (\n",
    "  SELECT *, current_timestamp() AS load_timestamp\n",
    "  FROM product_csv\n",
    ") AS source\n",
    "ON target.product_id = source.product_id\n",
    "\n",
    "WHEN MATCHED THEN\n",
    "  UPDATE SET\n",
    "    product_name   = source.product_name,\n",
    "    description    = source.description,\n",
    "    category       = source.category,\n",
    "    price          = source.price,\n",
    "    in_stock       = source.in_stock,\n",
    "    created_at     = source.created_at,\n",
    "    load_timestamp = source.load_timestamp\n",
    "\n",
    "WHEN NOT MATCHED THEN\n",
    "  INSERT (\n",
    "    product_id, product_name, description, category, price,\n",
    "    in_stock, created_at, load_timestamp\n",
    "  )\n",
    "  VALUES (\n",
    "    source.product_id, source.product_name, source.description, source.category,\n",
    "    source.price, source.in_stock, source.created_at, source.load_timestamp\n",
    "  );\n",
    "\n",
    "\"\"\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d7a82360-f503-46b9-919d-963fea36ff13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|      25|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" select count(*) from  bronze.product; \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a696c8a1-e28c-440a-9df9-d78d6eaf6570",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/08 17:35:50 WARN HiveConf: HiveConf of name hive.metastore.ssl.need.client.auth does not exist\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "MERGE INTO bronze.sales_order AS target\n",
    "USING (\n",
    "  SELECT \n",
    "    order_id,\n",
    "    customer_id,\n",
    "    order_date,\n",
    "    total_amount,\n",
    "    status,\n",
    "    current_timestamp() AS updated_at,\n",
    "    current_timestamp() AS load_timestamp\n",
    "  FROM sales_order_csv\n",
    ") AS source\n",
    "ON target.order_id = source.order_id\n",
    "\n",
    "WHEN MATCHED THEN\n",
    "  UPDATE SET\n",
    "    customer_id    = source.customer_id,\n",
    "    order_date     = source.order_date,\n",
    "    total_amount   = source.total_amount,\n",
    "    status         = source.status,\n",
    "    updated_at     = source.updated_at,\n",
    "    load_timestamp = source.load_timestamp\n",
    "\n",
    "WHEN NOT MATCHED THEN\n",
    "  INSERT (\n",
    "    order_id, customer_id, order_date, total_amount, status,\n",
    "    updated_at, load_timestamp\n",
    "  )\n",
    "  VALUES (\n",
    "    source.order_id, source.customer_id, source.order_date,\n",
    "    source.total_amount, source.status, source.updated_at, source.load_timestamp\n",
    "  );\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "653091ad-756c-4ca4-b4d1-86cd31b3a455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|    1000|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" select count(*) from  bronze.sales_order; \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a5221198-cd1f-48d7-9873-4dc521950e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "MERGE INTO bronze.order_item AS target\n",
    "USING (\n",
    "  SELECT \n",
    "    order_item_id,\n",
    "    order_id,\n",
    "    product_id,\n",
    "    quantity,\n",
    "    unit_price,\n",
    "    total_price,\n",
    "    current_timestamp() AS load_timestamp\n",
    "  FROM order_item_csv\n",
    ") AS source\n",
    "ON target.order_item_id = source.order_item_id\n",
    "\n",
    "WHEN MATCHED THEN\n",
    "  UPDATE SET\n",
    "    order_id       = source.order_id,\n",
    "    product_id     = source.product_id,\n",
    "    quantity       = source.quantity,\n",
    "    unit_price     = source.unit_price,\n",
    "    total_price    = source.total_price,\n",
    "    load_timestamp = source.load_timestamp\n",
    "\n",
    "WHEN NOT MATCHED THEN\n",
    "  INSERT (\n",
    "    order_item_id, order_id, product_id, quantity, unit_price, total_price, load_timestamp\n",
    "  )\n",
    "  VALUES (\n",
    "    source.order_item_id, source.order_id, source.product_id, source.quantity,\n",
    "    source.unit_price, source.total_price, source.load_timestamp\n",
    "  );\n",
    "\n",
    "\"\"\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4dec5eb7-5d66-4550-8ec9-2081234f1e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/08 23:53:59 WARN HiveConf: HiveConf of name hive.metastore.ssl.need.client.auth does not exist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|    3000|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" select count(*) from  bronze.order_item; \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce01993-38a8-486c-8784-322d28cd187f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
