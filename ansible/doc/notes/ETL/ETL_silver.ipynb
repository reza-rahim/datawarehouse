{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "002b38a5-f05d-41b7-9317-4e719d307947",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: Dspark.network.crypto.saslFallback\n",
      "Warning: Ignoring non-Spark config property: park.hadoop.hive.metastore.client.ssl.enabled\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/11 03:17:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/06/11 03:17:17 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "25/06/11 03:17:18 WARN S3ABlockOutputStream: Application invoked the Syncable API against stream writing to spark-events/app-20250611031712-0037.inprogress. This is unsupported\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "import os\n",
    "\n",
    "secret = os.environ.get(\"SPARK_AUTH_SECRET\")\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.authenticate\", \"true\")\n",
    "conf.set(\"spark.driver.extraJavaOptions\", f\"-Dspark.authenticate.secret={secret}\")\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf)\\\n",
    "    .appName(\"merge_silver\") \\\n",
    "    .master(\"spark://node1.dw.felicity.net.bd:7077,node2.dw.felicity.net.bd:7077\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "887a050a-1c22-4aec-bc40-797eaeda3586",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/11 03:17:23 WARN HiveConf: HiveConf of name hive.metastore.ssl.need.client.auth does not exist\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "MERGE INTO silver.dim_location AS target\n",
    "USING bronze.geo_location AS source\n",
    "ON target.location_id = source.location_id\n",
    "WHEN MATCHED THEN\n",
    "  UPDATE SET\n",
    "    target.country = source.country,\n",
    "    target.state = source.state,\n",
    "    target.city = source.city,\n",
    "    target.postal_code = source.postal_code,\n",
    "    target.load_timestamp = source.load_timestamp\n",
    "WHEN NOT MATCHED THEN\n",
    "  INSERT (location_id, country, state, city, postal_code, load_timestamp)\n",
    "  VALUES (source.location_id, source.country, source.state, source.city, source.postal_code, source.load_timestamp);\n",
    "\"\"\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1852767-2d46-4dbf-aba7-799100788f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/11 03:43:48 WARN HiveConf: HiveConf of name hive.metastore.ssl.need.client.auth does not exist\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "MERGE INTO silver.dim_customer AS target\n",
    "USING bronze.customer AS source\n",
    "ON target.customer_id = source.customer_id\n",
    "WHEN MATCHED THEN\n",
    "  UPDATE SET\n",
    "    target.full_name = source.full_name,\n",
    "    target.email = source.email,\n",
    "    target.phone_number = source.phone_number,\n",
    "    target.location_id = source.location_id,\n",
    "    target.created_at = source.created_at,\n",
    "    target.load_timestamp = source.load_timestamp\n",
    "WHEN NOT MATCHED THEN\n",
    "  INSERT (\n",
    "    customer_id, full_name, email, phone_number,\n",
    "    location_id, created_at, load_timestamp\n",
    "  )\n",
    "  VALUES (\n",
    "    source.customer_id, source.full_name, source.email, source.phone_number,\n",
    "    source.location_id, source.created_at, source.load_timestamp\n",
    "  );\n",
    "\n",
    "\n",
    "\"\"\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a765bd42-3aaa-41dd-be3d-d0efe0fe05d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "MERGE INTO silver.dim_product AS target\n",
    "USING bronze.product AS source\n",
    "ON target.product_id = source.product_id\n",
    "WHEN MATCHED THEN\n",
    "  UPDATE SET\n",
    "    target.product_name = source.product_name,\n",
    "    target.description = source.description,\n",
    "    target.category = source.category,\n",
    "    target.price = source.price,\n",
    "    target.in_stock = source.in_stock,\n",
    "    target.created_at = source.created_at,\n",
    "    target.load_timestamp = source.load_timestamp\n",
    "WHEN NOT MATCHED THEN\n",
    "  INSERT (\n",
    "    product_id, product_name, description, category,\n",
    "    price, in_stock, created_at, load_timestamp\n",
    "  )\n",
    "  VALUES (\n",
    "    source.product_id, source.product_name, source.description, source.category,\n",
    "    source.price, source.in_stock, source.created_at, source.load_timestamp\n",
    "  );\n",
    "\n",
    "\n",
    "\"\"\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e40420f9-2aab-4dab-82c3-13cf507c92b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TEMPORARY VIEW sales_fact_staging AS\n",
    "SELECT\n",
    "    oi.order_item_id,\n",
    "    oi.order_id,\n",
    "    so.customer_id,\n",
    "    oi.product_id,\n",
    "    cu.location_id,\n",
    "    so.order_date,\n",
    "    so.updated_at,\n",
    "    oi.quantity,\n",
    "    oi.unit_price,\n",
    "    oi.total_price,\n",
    "    so.status,\n",
    "    oi.load_timestamp\n",
    "FROM bronze.order_item oi\n",
    "JOIN bronze.sales_order so ON oi.order_id = so.order_id\n",
    "JOIN bronze.customer cu ON so.customer_id = cu.customer_id;\n",
    "\n",
    "\"\"\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34603ae0-4c97-48d4-b59c-d316aa5800eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/11 03:58:19 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "MERGE INTO silver.fact_sales AS target\n",
    "USING sales_fact_staging AS source\n",
    "ON target.order_item_id = source.order_item_id\n",
    "WHEN MATCHED THEN\n",
    "  UPDATE SET\n",
    "    target.order_id = source.order_id,\n",
    "    target.customer_id = source.customer_id,\n",
    "    target.product_id = source.product_id,\n",
    "    target.location_id = source.location_id,\n",
    "    target.order_date = source.order_date,\n",
    "    target.updated_at = source.updated_at,\n",
    "    target.quantity = source.quantity,\n",
    "    target.unit_price = source.unit_price,\n",
    "    target.total_price = source.total_price,\n",
    "    target.status = source.status,\n",
    "    target.load_timestamp = source.load_timestamp\n",
    "WHEN NOT MATCHED THEN\n",
    "  INSERT (\n",
    "    order_item_id, order_id, customer_id, product_id,\n",
    "    location_id, order_date, updated_at,\n",
    "    quantity, unit_price, total_price, status, load_timestamp\n",
    "  )\n",
    "  VALUES (\n",
    "    source.order_item_id, source.order_id, source.customer_id, source.product_id,\n",
    "    source.location_id, source.order_date, source.updated_at,\n",
    "    source.quantity, source.unit_price, source.total_price, source.status, source.load_timestamp\n",
    "  );\n",
    "\n",
    "\n",
    "\"\"\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94311ed5-16ef-449e-9c07-0d247ac27b69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
